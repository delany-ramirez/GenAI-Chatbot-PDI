# ğŸ¤– Chatbot GenAI - Caso de Estudio PDI-UTP  

> **Este fork se basa en el repositorio original de [GenAIOps_Pycon2025](https://github.com/darkanita/GenAIOps_Pycon2025) de @darkanita.**

*(versiÃ³n local con Ollama + RTX 4060 Ti)*

Este proyecto demuestra cÃ³mo construir, evaluar y automatizar un chatbot **RAG** (Retrieval Augmented Generation) siguiendo buenas prÃ¡cticas de **GenAIOps**, **sin depender de la API de OpenAI**: todos los modelos (LLM + embeddings) se ejecutan en tu propia GPU mediante **Ollama**.

---

## ğŸ§  Caso de Estudio
El chatbot responde preguntas sobre el **Plan de Desarrollo Institucional** de la Universidad TecnolÃ³gica de Pereira a partir de documentos PDF internos.

---

## ğŸ“‚ Estructura del proyecto
```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ui_streamlit.py           â† interfaz simple del chatbot
â”‚   â”œâ”€â”€ main_interface.py         â† interfaz combinada con mÃ©tricas
â”‚   â”œâ”€â”€ run_eval.py               â† evaluaciÃ³n automÃ¡tica
â”‚   â”œâ”€â”€ rag_pipeline.py           â† lÃ³gica de ingestiÃ³n y RAG
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ v1_asistente_rrhh.txt
â”‚       â””â”€â”€ v2_resumido_directo.txt
â”œâ”€â”€ data/pdfs/                    â† documentos fuente
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_run_eval.py
â”‚   â”œâ”€â”€ eval_questions_pdi.json         â† dataset de evaluaciÃ³n
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ eval.yml
â”‚   â””â”€â”€ test.yml
```

---

## ğŸš¦ Ciclo de vida GenAIOps aplicado

### 1. ğŸ§± PreparaciÃ³n del entorno

```bash
# 1-a. clona el repo
git clone https://github.com/delany-ramirez/GenAI-Chatbot-PDI chatbot-pdi
cd chatbot-pdi

# 1-b. crea el entorno
conda create -n chatbot-pdi python=3.10 -y
conda activate chatbot-pdi
pip install -r requirements.txt      # incluye langchain-ollama

# 1-c. instala Ollama (solo una vez)
winget install --id Ollama.Ollama -e     # Windows
# curl -fsSL https://ollama.ai/install.sh | sh   # macOS / Linux

# 1-d. descarga los modelos locales
ollama pull llama3:8b           # LLM principal
ollama pull nomic-embed-text    # modelo de embeddings

# 1-e. variables opcionales
cp .env.example .env
# Abre .env y ajusta:
# OLLAMA_BASE_URL=http://localhost:11434
```

---

### 2. ğŸ” Ingesta y vectorizaciÃ³n

```bash
python -m app.rag_pipeline --rebuild_index
```

El script:

1. Carga PDFs de `data/pdfs/`.  
2. Trocea en *chunks* (512/50 por defecto).  
3. Genera *embeddings* con **`nomic-embed-text`** vÃ­a Ollama.  
4. Guarda un Ã­ndice **FAISS** en `data/vectorstore/`.  
5. Registra parÃ¡metros en **MLflow**.

ParÃ¡metros personalizables:
```python
save_vectorstore(chunk_size=1024, chunk_overlap=100)
```

---

### 3. ğŸ§  Pipeline RAG

```python
from app.rag_pipeline import load_vectorstore_from_disk, build_chain
vectordb = load_vectorstore_from_disk()
chain    = build_chain(vectordb, prompt_version="v3_asistente_pdi")
```

* Usa **\`ChatOllama\`** como LLM y el FAISS como *retriever*.

---

### 4. ğŸ’¬ Interfaz Streamlit

```bash
streamlit run app/ui_streamlit.py        # UI bÃ¡sica
# Ã³
streamlit run app/main_interface.py      # UI + mÃ©tricas
```

---

### 5. ğŸ§ª EvaluaciÃ³n automÃ¡tica

```bash
python app/run_eval.py
```

* Genera respuestas con el RAG local.  
* EvalÃºa con **LangChain Eval** (`QAEvalChain`) usando **ChatOllama**.  
* Registra mÃ©tricas en **MLflow**.

---

### 6. ğŸ“ˆ Dashboard de resultados

```bash
streamlit run app/dashboard.py
```

---

### 7. ğŸ” AutomatizaciÃ³n CI (GitHub Actions)

* `.github/workflows/eval.yml` â€“ evaluaciÃ³n automÃ¡tica.  
* `.github/workflows/test.yml` â€“ pruebas unitarias.

---

### 8. ğŸ§ª ValidaciÃ³n

```bash
pytest tests/test_run_eval.py
```
Exige â‰¥ 80 % de precisiÃ³n con el dataset base.

---

## âš™ï¸ Stack tecnolÃ³gico

| Capa              | TecnologÃ­a                                     |
|-------------------|------------------------------------------------|
| **LLM**           | \`llama3:8b\` (vÃ­a **Ollama**)                   |
| **Embeddings**    | \`nomic-embed-text\` (**Ollama**)                |
| **RAG framework** | **LangChain** + **FAISS**                      |
| **UI / MLOps**    | **Streamlit**, **MLflow**, **GitHub Actions**  |


---

## ğŸ› ï¸ Requisitos de hardware

* GPU con â‰¥ 8 GB VRAM (RTX 4060 Ti 16 GB recomendada).  
* Driver NVIDIA 550+ y CUDA â‰¥ 12 (Ollama incluye cuBLAS).  
* El modelo `llama3:8b` consume ~7 GB; ajusta `OLLAMA_NUM_GPU_LAYERS` si necesitas limitar VRAM.

---

## ğŸ“ DesafÃ­o para estudiantes
ğŸ§© Parte 1: PersonalizaciÃ³n

1. Elige un nuevo dominio
Ejemplos: salud, educaciÃ³n, legal, bancario, etc.

2. Reemplaza los documentos PDF
UbÃ­calos en data/pdfs/.

3. Modifica o crea tus prompts
Edita los archivos en app/prompts/.

4. Crea un conjunto de pruebas
En tests/eval_dataset.json, define preguntas y respuestas esperadas para evaluar a tu chatbot.

âœ… Parte 2: EvaluaciÃ³n AutomÃ¡tica

1. Ejecuta run_eval.py para probar tu sistema actual.
Actualmente, la evaluaciÃ³n estÃ¡ basada en QAEvalChain de LangChain, que devuelve una mÃ©trica binaria: correcto / incorrecto.

ğŸ”§ Parte 3: Â¡Tu reto! (ğŸ‘¨â€ğŸ”¬ nivel investigador)

1. Mejora el sistema de evaluaciÃ³n:

    * Agrega evaluaciÃ³n con LabeledCriteriaEvalChain usando al menos los siguientes criterios:

        * "correctness" â€“ Â¿Es correcta la respuesta?
        * "relevance" â€“ Â¿Es relevante respecto a la pregunta?
        * "coherence" â€“ Â¿EstÃ¡ bien estructurada la respuesta?
        * "toxicity" â€“ Â¿Contiene lenguaje ofensivo o riesgoso?
        * "harmfulness" â€“ Â¿PodrÃ­a causar daÃ±o la informaciÃ³n?

    * Cada criterio debe registrar:

        * Una mÃ©trica en MLflow (score)

    * Y opcionalmente, un razonamiento como artefacto (reasoning)

    ğŸ“š Revisa la [documentaciÃ³n de LabeledCriteriaEvalChain](https://python.langchain.com/api_reference/langchain/evaluation/langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain.html) para implementarlo.

ğŸ“Š Parte 4: Mejora el dashboard

1. Extiende dashboard.py o main_interface.py para visualizar:

    * Las mÃ©tricas por criterio (correctness_score, toxicity_score, etc.).
    * Una opciÃ³n para seleccionar y comparar diferentes criterios en grÃ¡ficos.
    * (Opcional) Razonamientos del modelo como texto.    

ğŸ§ª Parte 5: Presenta y reflexiona
1. Compara configuraciones distintas (chunk size, prompt) y justifica tu selecciÃ³n.
    * Â¿CuÃ¡l configuraciÃ³n genera mejores respuestas?
    * Â¿En quÃ© fallan los modelos? Â¿Fueron tÃ³xicos o incoherentes?
    * Usa evidencias desde MLflow y capturas del dashboard.

ğŸš€ Bonus

- Â¿Te animas a crear un nuevo criterio como "claridad" o "creatividad"? Puedes definirlo tÃº mismo y usarlo con LabeledCriteriaEvalChain.

---

Â¡Listo para ser usado en clase, investigaciÃ³n o producciÃ³n educativa! ğŸš€